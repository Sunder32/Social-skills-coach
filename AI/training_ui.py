"""
Gradio Training Interface
Web UI for training and managing the Social Skills Model
"""
import gradio as gr
import torch
import os
import json
import threading
from datetime import datetime
from typing import Optional, List
import traceback
import shutil

# Import training components
from config import config
from core.model import SocialSkillsModel
from core.llm_client import LLMClient
from training.trainer import SFTTrainer
from training.dpo_trainer import DPOTrainer
from training.dataset import ConversationDataset, PreferenceDataset, create_sample_data
from training.document_loader import DocumentLoader, process_books_for_training
from rag.retriever import RAGRetriever


# Global state
training_state = {
    "is_training": False,
    "progress": 0,
    "current_step": 0,
    "total_steps": 0,
    "loss": 0,
    "status": "–ì–æ—Ç–æ–≤ –∫ –æ–±—É—á–µ–Ω–∏—é",
    "logs": []
}

model = None
tokenizer = None


def log_message(message: str):
    """Add message to logs"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    training_state["logs"].append(f"[{timestamp}] {message}")
    # Keep only last 100 logs
    training_state["logs"] = training_state["logs"][-100:]


def get_model_info():
    """Get information about current model"""
    global model
    
    model_path = config.model.model_path
    
    info = {
        "–°—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏": "–ù–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞",
        "–ü—É—Ç—å": model_path,
        "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã": "-",
        "–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ": config.training.device,
        "GPU –¥–æ—Å—Ç—É–ø–µ–Ω": torch.cuda.is_available()
    }
    
    if os.path.exists(model_path):
        info["–°—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏"] = "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –Ω–∞ –¥–∏—Å–∫–µ"
        
        config_path = os.path.join(model_path, "config.json")
        if os.path.exists(config_path):
            with open(config_path) as f:
                model_config = json.load(f)
            info["Hidden size"] = model_config.get("hidden_size", "-")
            info["Layers"] = model_config.get("num_layers", "-")
            info["Heads"] = model_config.get("num_heads", "-")
    
    if model is not None:
        info["–°—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏"] = "–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ –ø–∞–º—è—Ç—å"
        info["–ü–∞—Ä–∞–º–µ—Ç—Ä—ã"] = f"{model.get_num_params():,}"
        info["Trainable"] = f"{model.get_num_trainable_params():,}"
    
    return "\n".join([f"**{k}:** {v}" for k, v in info.items()])


def load_model_handler():
    """Load model into memory"""
    global model, tokenizer
    
    try:
        from transformers import AutoTokenizer
        
        log_message("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
        
        model_path = config.model.model_path
        
        if os.path.exists(model_path):
            model = SocialSkillsModel.from_pretrained(
                model_path, 
                device=config.training.device if torch.cuda.is_available() else "cpu"
            )
            tokenizer = AutoTokenizer.from_pretrained(config.model.tokenizer_path)
        else:
            log_message("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é...")
            model = SocialSkillsModel(
                vocab_size=config.model.vocab_size,
                hidden_size=config.model.hidden_size,
                num_layers=config.model.num_layers,
                num_heads=config.model.num_heads,
                max_length=config.model.max_length
            )
            
            device = config.training.device if torch.cuda.is_available() else "cpu"
            model.to(device)
            
            # Load tokenizer
            tokenizer = AutoTokenizer.from_pretrained("ai-forever/rugpt3small_based_on_gpt2")
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
        
        log_message(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞! –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {model.get_num_params():,}")
        return get_model_info(), "\n".join(training_state["logs"])
    
    except Exception as e:
        log_message(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)}")
        return get_model_info(), "\n".join(training_state["logs"])


def create_sample_data_handler():
    """Create sample training data"""
    try:
        sft_path, pref_path = create_sample_data(config.data_dir)
        log_message(f"–°–æ–∑–¥–∞–Ω—ã –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:")
        log_message(f"  SFT: {sft_path}")
        log_message(f"  DPO: {pref_path}")
        return "\n".join(training_state["logs"])
    except Exception as e:
        log_message(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        return "\n".join(training_state["logs"])


def progress_callback(step, total, metrics):
    """Update training progress"""
    training_state["current_step"] = step
    training_state["total_steps"] = total
    training_state["progress"] = int(step / total * 100) if total > 0 else 0
    training_state["loss"] = metrics.get("loss", 0)


def train_sft_handler(
    data_path: str,
    num_epochs: int,
    batch_size: int,
    learning_rate: float,
    warmup_steps: int
):
    """Start SFT training"""
    global model, tokenizer
    
    if training_state["is_training"]:
        return "–û–±—É—á–µ–Ω–∏–µ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–æ!", "\n".join(training_state["logs"]), get_model_info()
    
    if model is None:
        return "–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å!", "\n".join(training_state["logs"]), get_model_info()
    
    if not os.path.exists(data_path):
        return f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_path}", "\n".join(training_state["logs"]), get_model_info()
    
    def train_thread():
        try:
            training_state["is_training"] = True
            training_state["status"] = "–û–±—É—á–µ–Ω–∏–µ SFT..."
            log_message("–ù–∞—á–∞–ª–æ SFT –æ–±—É—á–µ–Ω–∏—è")
            
            # Create dataset
            dataset = ConversationDataset(
                data_path=data_path,
                tokenizer=tokenizer,
                max_length=config.model.max_length
            )
            
            if len(dataset) == 0:
                log_message("–û—à–∏–±–∫–∞: –¥–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç!")
                training_state["is_training"] = False
                return
            
            log_message(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
            
            # Create trainer
            trainer = SFTTrainer(
                model=model,
                tokenizer=tokenizer,
                train_dataset=dataset,
                output_dir=config.training.checkpoint_dir
            )
            
            # Train
            history = trainer.train(
                num_epochs=int(num_epochs),
                batch_size=int(batch_size),
                learning_rate=float(learning_rate),
                warmup_steps=int(warmup_steps),
                progress_callback=progress_callback
            )
            
            log_message("SFT –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            log_message(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {config.training.checkpoint_dir}")
            
        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {str(e)}")
            log_message(traceback.format_exc())
        finally:
            training_state["is_training"] = False
            training_state["status"] = "–ì–æ—Ç–æ–≤ –∫ –æ–±—É—á–µ–Ω–∏—é"
    
    # Start training in background thread
    thread = threading.Thread(target=train_thread)
    thread.start()
    
    return "–û–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ! –°–ª–µ–¥–∏—Ç–µ –∑–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º –≤ –ª–æ–≥–∞—Ö.", "\n".join(training_state["logs"]), get_model_info()


def train_dpo_handler(
    data_path: str,
    num_epochs: int,
    batch_size: int,
    learning_rate: float,
    beta: float
):
    """Start DPO training"""
    global model, tokenizer
    
    if training_state["is_training"]:
        return "–û–±—É—á–µ–Ω–∏–µ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–æ!", "\n".join(training_state["logs"]), get_model_info()
    
    if model is None:
        return "–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å!", "\n".join(training_state["logs"]), get_model_info()
    
    def train_thread():
        try:
            training_state["is_training"] = True
            training_state["status"] = "–û–±—É—á–µ–Ω–∏–µ DPO..."
            log_message("–ù–∞—á–∞–ª–æ DPO –æ–±—É—á–µ–Ω–∏—è")
            
            # Create reference model (copy of current model)
            ref_model = SocialSkillsModel(
                vocab_size=model.vocab_size,
                hidden_size=model.hidden_size,
                num_layers=model.num_layers,
                num_heads=model.num_heads,
                max_length=model.max_length
            )
            ref_model.load_state_dict(model.state_dict())
            
            # Create dataset
            dataset = PreferenceDataset(
                data_path=data_path,
                tokenizer=tokenizer,
                max_length=config.model.max_length
            )
            
            if len(dataset) == 0:
                log_message("–û—à–∏–±–∫–∞: –¥–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç!")
                training_state["is_training"] = False
                return
            
            log_message(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π")
            
            # Create trainer
            trainer = DPOTrainer(
                model=model,
                ref_model=ref_model,
                tokenizer=tokenizer,
                train_dataset=dataset,
                output_dir=config.training.checkpoint_dir,
                beta=float(beta)
            )
            
            # Train
            history = trainer.train(
                num_epochs=int(num_epochs),
                batch_size=int(batch_size),
                learning_rate=float(learning_rate),
                progress_callback=progress_callback
            )
            
            log_message("DPO –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            
        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ DPO: {str(e)}")
            log_message(traceback.format_exc())
        finally:
            training_state["is_training"] = False
            training_state["status"] = "–ì–æ—Ç–æ–≤ –∫ –æ–±—É—á–µ–Ω–∏—é"
    
    thread = threading.Thread(target=train_thread)
    thread.start()
    
    return "DPO –æ–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ!", "\n".join(training_state["logs"]), get_model_info()


def save_model_handler():
    """Save current model"""
    global model, tokenizer
    
    if model is None:
        return "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!", "\n".join(training_state["logs"])
    
    try:
        save_path = config.model.model_path
        model.save_pretrained(save_path)
        
        tokenizer_path = config.model.tokenizer_path
        os.makedirs(tokenizer_path, exist_ok=True)
        tokenizer.save_pretrained(tokenizer_path)
        
        log_message(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {save_path}")
        return f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {save_path}", "\n".join(training_state["logs"])
    
    except Exception as e:
        log_message(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {str(e)}")
        return f"–û—à–∏–±–∫–∞: {str(e)}", "\n".join(training_state["logs"])


def test_generation_handler(prompt: str, max_tokens: int, temperature: float):
    """Test model generation"""
    global model, tokenizer
    
    if model is None:
        return "–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å!"
    
    try:
        model.eval()
        
        # Tokenize
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=config.model.max_length - max_tokens
        )
        
        device = next(model.parameters()).device
        input_ids = inputs["input_ids"].to(device)
        
        # Generate
        with torch.no_grad():
            outputs = model.generate(
                input_ids=input_ids,
                max_new_tokens=int(max_tokens),
                temperature=float(temperature),
                top_p=0.9,
                top_k=50
            )
        
        # Decode
        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        return generated
    
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"


def get_training_status():
    """Get current training status"""
    status = f"""
**–°—Ç–∞—Ç—É—Å:** {training_state['status']}
**–û–±—É—á–µ–Ω–∏–µ:** {'–î–∞' if training_state['is_training'] else '–ù–µ—Ç'}
**–ü—Ä–æ–≥—Ä–µ—Å—Å:** {training_state['progress']}%
**–®–∞–≥:** {training_state['current_step']} / {training_state['total_steps']}
**Loss:** {training_state['loss']:.4f}
"""
    return status


def refresh_logs():
    """Refresh logs display"""
    return "\n".join(training_state["logs"][-50:])


def refresh_status():
    """Refresh all status displays"""
    return (
        get_training_status(),
        "\n".join(training_state["logs"][-50:]),
        get_model_info()
    )


# =========================
# Document Training Handlers
# =========================

rag_retriever = None


def process_documents_handler(
    files: List,
    chunk_size: int,
    data_type: str
):
    """Process uploaded documents for training"""
    global rag_retriever
    
    if not files:
        return "–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã!", "\n".join(training_state["logs"])
    
    try:
        log_message(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(files)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        
        loader = DocumentLoader(chunk_size=int(chunk_size))
        documents = []
        
        for file in files:
            try:
                text, metadata = loader.load_file(file.name)
                if text.strip():
                    documents.append((text, metadata))
                    log_message(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω: {metadata['filename']} ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
                else:
                    log_message(f"‚ö† –ü—É—Å—Ç–æ–π —Ñ–∞–π–ª: {metadata['filename']}")
            except Exception as e:
                log_message(f"‚úó –û—à–∏–±–∫–∞ {file.name}: {str(e)}")
        
        if not documents:
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞!", "\n".join(training_state["logs"])
        
        # Create output directory
        output_dir = os.path.join(config.data_dir, "documents")
        os.makedirs(output_dir, exist_ok=True)
        
        # Save training data
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if data_type == "knowledge":
            output_path = os.path.join(output_dir, f"knowledge_{timestamp}.jsonl")
            loader.create_training_data(documents, output_path, data_type="knowledge")
            
            # Also add to RAG index
            log_message("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ RAG –∏–Ω–¥–µ–∫—Å...")
            if rag_retriever is None:
                rag_retriever = RAGRetriever()
            
            # Convert to RAG format
            rag_docs = []
            for text, metadata in documents:
                rag_docs.append({
                    "content": text,
                    "title": metadata.get("filename", ""),
                    "topic": "",
                    "source": metadata.get("source", "")
                })
            
            # Run async method
            import asyncio
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            loop.run_until_complete(rag_retriever.add_documents(rag_docs))
            
            log_message(f"‚úì –î–æ–∫—É–º–µ–Ω—Ç—ã –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ RAG –∏–Ω–¥–µ–∫—Å")
            
        elif data_type == "sft":
            output_path = os.path.join(output_dir, f"sft_{timestamp}.jsonl")
            loader.create_training_data(documents, output_path, data_type="conversation")
            
        elif data_type == "qa":
            output_path = os.path.join(output_dir, f"qa_{timestamp}.jsonl")
            loader.create_training_data(documents, output_path, data_type="qa")
        
        log_message(f"‚úì –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")
        
        result = f"""
‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}
üìÅ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}
üìä –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: {data_type}
"""
        return result, "\n".join(training_state["logs"])
    
    except Exception as e:
        log_message(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")
        log_message(traceback.format_exc())
        return f"–û—à–∏–±–∫–∞: {str(e)}", "\n".join(training_state["logs"])


def process_directory_handler(
    dir_path: str,
    chunk_size: int,
    data_type: str
):
    """Process all documents in a directory"""
    global rag_retriever
    
    if not dir_path or not os.path.exists(dir_path):
        return "–£–∫–∞–∂–∏—Ç–µ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é!", "\n".join(training_state["logs"])
    
    try:
        log_message(f"–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {dir_path}")
        
        loader = DocumentLoader(chunk_size=int(chunk_size))
        documents = loader.load_directory(dir_path)
        
        if not documents:
            return "–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤!", "\n".join(training_state["logs"])
        
        log_message(f"–ù–∞–π–¥–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        for text, metadata in documents:
            log_message(f"  ‚úì {metadata['filename']}")
        
        # Create output directory
        output_dir = os.path.join(config.data_dir, "documents")
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = os.path.join(output_dir, f"{data_type}_{timestamp}.jsonl")
        
        loader.create_training_data(documents, output_path, data_type=data_type)
        
        # Add to RAG if knowledge type
        if data_type == "knowledge":
            log_message("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ RAG –∏–Ω–¥–µ–∫—Å...")
            if rag_retriever is None:
                rag_retriever = RAGRetriever()
            
            # Convert to RAG format
            rag_docs = []
            for text, metadata in documents:
                rag_docs.append({
                    "content": text,
                    "title": metadata.get("filename", ""),
                    "topic": "",
                    "source": metadata.get("source", "")
                })
            
            # Run async method
            import asyncio
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            loop.run_until_complete(rag_retriever.add_documents(rag_docs))
            
            log_message("‚úì –î–æ–∫—É–º–µ–Ω—Ç—ã –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ RAG")
        
        log_message(f"‚úì –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {output_path}")
        
        result = f"""
‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}
üìÅ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}
"""
        return result, "\n".join(training_state["logs"])
    
    except Exception as e:
        log_message(f"–û—à–∏–±–∫–∞: {str(e)}")
        return f"–û—à–∏–±–∫–∞: {str(e)}", "\n".join(training_state["logs"])


def train_on_documents_handler(
    data_path: str,
    num_epochs: int,
    batch_size: int,
    learning_rate: float
):
    """Train model on processed documents"""
    global model, tokenizer
    
    if training_state["is_training"]:
        return "–û–±—É—á–µ–Ω–∏–µ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–æ!", "\n".join(training_state["logs"]), get_model_info()
    
    if model is None:
        return "–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å!", "\n".join(training_state["logs"]), get_model_info()
    
    # Handle directory path - find latest .jsonl file
    actual_path = data_path
    if os.path.isdir(data_path):
        jsonl_files = [f for f in os.listdir(data_path) if f.endswith('.jsonl')]
        if not jsonl_files:
            return f"–í –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {data_path} –Ω–µ—Ç .jsonl —Ñ–∞–π–ª–æ–≤!", "\n".join(training_state["logs"]), get_model_info()
        # Get the most recent file
        jsonl_files.sort(reverse=True)
        actual_path = os.path.join(data_path, jsonl_files[0])
        log_message(f"–í—ã–±—Ä–∞–Ω —Ñ–∞–π–ª: {jsonl_files[0]}")
    
    if not os.path.exists(actual_path):
        return f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {actual_path}", "\n".join(training_state["logs"]), get_model_info()
    
    def train_thread():
        try:
            training_state["is_training"] = True
            training_state["status"] = "–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö..."
            log_message("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö")
            log_message(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö: {actual_path}")
            
            # Create dataset
            dataset = ConversationDataset(
                data_path=actual_path,
                tokenizer=tokenizer,
                max_length=config.model.max_length
            )
            
            if len(dataset) == 0:
                log_message("–û—à–∏–±–∫–∞: –¥–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç!")
                training_state["is_training"] = False
                return
            
            log_message(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            # Create trainer
            trainer = SFTTrainer(
                model=model,
                tokenizer=tokenizer,
                train_dataset=dataset,
                output_dir=config.training.checkpoint_dir
            )
            
            # Train
            history = trainer.train(
                num_epochs=int(num_epochs),
                batch_size=int(batch_size),
                learning_rate=float(learning_rate),
                warmup_steps=50,
                progress_callback=progress_callback
            )
            
            log_message("‚úì –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            log_message(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {config.training.checkpoint_dir}")
            
        except Exception as e:
            log_message(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {str(e)}")
            log_message(traceback.format_exc())
        finally:
            training_state["is_training"] = False
            training_state["status"] = "–ì–æ—Ç–æ–≤ –∫ –æ–±—É—á–µ–Ω–∏—é"
    
    thread = threading.Thread(target=train_thread)
    thread.start()
    
    return "–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –∑–∞–ø—É—â–µ–Ω–æ!", "\n".join(training_state["logs"]), get_model_info()


def list_document_datasets():
    """List available document datasets"""
    doc_dir = os.path.join(config.data_dir, "documents")
    
    if not os.path.exists(doc_dir):
        return "–ù–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
    
    files = [f for f in os.listdir(doc_dir) if f.endswith('.jsonl')]
    
    if not files:
        return "–ù–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
    
    result = "**–î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã:**\n\n"
    for f in sorted(files, reverse=True):
        path = os.path.join(doc_dir, f)
        size = os.path.getsize(path)
        # Count lines
        with open(path, 'r', encoding='utf-8') as file:
            lines = sum(1 for _ in file)
        result += f"- `{f}` ({lines} –ø—Ä–∏–º–µ—Ä–æ–≤, {size // 1024} KB)\n"
    
    return result


def search_rag_handler(query: str, top_k: int):
    """Search in RAG knowledge base"""
    global rag_retriever
    
    if rag_retriever is None:
        try:
            rag_retriever = RAGRetriever()
        except:
            return "RAG –∏–Ω–¥–µ–∫—Å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã."
    
    try:
        # Run async method
        import asyncio
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        results = loop.run_until_complete(rag_retriever.search(query, top_k=int(top_k)))
        
        if not results:
            return "–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
        
        output = f"**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è:** {query}\n\n"
        
        for i, result in enumerate(results, 1):
            content = result.get('content', result.get('text', ''))
            score = result.get('score', 0)
            source = result.get('title', result.get('source', 'Unknown'))
            output += f"---\n**{i}. Score: {score:.4f}** (–∏–∑ {source})\n\n"
            output += content[:500] + ("..." if len(content) > 500 else "") + "\n\n"
        
        return output
    
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {str(e)}"


# Build Gradio Interface
def create_interface():
    """Create Gradio interface"""
    
    with gr.Blocks(
        title="Social Skills Model Training",
        theme=gr.themes.Soft()
    ) as interface:
        
        gr.Markdown("""
        # üéØ Social Skills Model - Training Interface
        
        –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤ –Ω–∞ PyTorch.
        """)
        
        with gr.Row():
            with gr.Column(scale=2):
                # Model Info
                with gr.Group():
                    gr.Markdown("### üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏")
                    model_info = gr.Markdown(get_model_info())
                    
                    with gr.Row():
                        load_btn = gr.Button("üîÑ –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å", variant="primary")
                        save_btn = gr.Button("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å")
                
                # Training Status
                with gr.Group():
                    gr.Markdown("### üìà –°—Ç–∞—Ç—É—Å –æ–±—É—á–µ–Ω–∏—è")
                    status_display = gr.Markdown(get_training_status())
                    refresh_btn = gr.Button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å")
            
            with gr.Column(scale=3):
                # Logs
                with gr.Group():
                    gr.Markdown("### üìã –õ–æ–≥–∏")
                    logs_display = gr.Textbox(
                        value="",
                        lines=15,
                        max_lines=20,
                        label="",
                        interactive=False
                    )
        
        with gr.Tabs():
            # SFT Training Tab
            with gr.Tab("üéì SFT –û–±—É—á–µ–Ω–∏–µ"):
                gr.Markdown("""
                **Supervised Fine-Tuning** - –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö –¥–∏–∞–ª–æ–≥–æ–≤.
                –§–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö: JSONL —Å –ø–æ–ª—è–º–∏ `instruction`, `input`, `output`.
                """)
                
                with gr.Row():
                    sft_data_path = gr.Textbox(
                        label="–ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º",
                        value="./data/train/sft_data.jsonl",
                        placeholder="–ü—É—Ç—å –∫ JSONL —Ñ–∞–π–ª—É"
                    )
                
                with gr.Row():
                    sft_epochs = gr.Slider(1, 10, value=3, step=1, label="–≠–ø–æ—Ö–∏")
                    sft_batch = gr.Slider(1, 32, value=4, step=1, label="Batch Size")
                    sft_lr = gr.Number(value=2e-5, label="Learning Rate")
                    sft_warmup = gr.Slider(0, 1000, value=100, step=10, label="Warmup Steps")
                
                with gr.Row():
                    create_data_btn = gr.Button("üìù –°–æ–∑–¥–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö")
                    train_sft_btn = gr.Button("üöÄ –ù–∞—á–∞—Ç—å SFT –æ–±—É—á–µ–Ω–∏–µ", variant="primary")
                
                sft_result = gr.Textbox(label="–†–µ–∑—É–ª—å—Ç–∞—Ç", lines=2)
            
            # DPO Training Tab
            with gr.Tab("‚öñÔ∏è DPO –û–±—É—á–µ–Ω–∏–µ"):
                gr.Markdown("""
                **Direct Preference Optimization** - –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö.
                –§–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö: JSONL —Å –ø–æ–ª—è–º–∏ `prompt`, `chosen`, `rejected`.
                """)
                
                with gr.Row():
                    dpo_data_path = gr.Textbox(
                        label="–ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º",
                        value="./data/train/preference_data.jsonl",
                        placeholder="–ü—É—Ç—å –∫ JSONL —Ñ–∞–π–ª—É"
                    )
                
                with gr.Row():
                    dpo_epochs = gr.Slider(1, 5, value=1, step=1, label="–≠–ø–æ—Ö–∏")
                    dpo_batch = gr.Slider(1, 16, value=2, step=1, label="Batch Size")
                    dpo_lr = gr.Number(value=1e-5, label="Learning Rate")
                    dpo_beta = gr.Slider(0.01, 0.5, value=0.1, step=0.01, label="Beta")
                
                train_dpo_btn = gr.Button("üöÄ –ù–∞—á–∞—Ç—å DPO –æ–±—É—á–µ–Ω–∏–µ", variant="primary")
                dpo_result = gr.Textbox(label="–†–µ–∑—É–ª—å—Ç–∞—Ç", lines=2)
            
            # Document Training Tab
            with gr.Tab("üìö –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö"):
                gr.Markdown("""
                ### –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–Ω–∏–≥–∞—Ö –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
                
                –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: **PDF, DOCX, TXT, EPUB, Markdown**
                
                1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–ª–∏ —É–∫–∞–∂–∏—Ç–µ –ø–∞–ø–∫—É
                2. –í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—Ä–∞–±–æ—Ç–∫—É
                4. –ù–∞—á–Ω–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                """)
                
                with gr.Tabs():
                    with gr.Tab("üì§ –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤"):
                        doc_files = gr.File(
                            label="–ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã",
                            file_count="multiple",
                            file_types=[".pdf", ".docx", ".doc", ".txt", ".epub", ".md"]
                        )
                        
                        with gr.Row():
                            doc_chunk_size = gr.Slider(
                                200, 2000, 
                                value=1000, 
                                step=100, 
                                label="–†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ (—Å–∏–º–≤–æ–ª–æ–≤)"
                            )
                            doc_data_type = gr.Dropdown(
                                choices=["knowledge", "sft", "qa"],
                                value="knowledge",
                                label="–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö",
                                info="knowledge - –¥–ª—è RAG, sft - –¥–ª—è fine-tuning, qa - –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç"
                            )
                        
                        process_files_btn = gr.Button("üîÑ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª—ã", variant="primary")
                        doc_process_result = gr.Textbox(label="–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏", lines=4)
                    
                    with gr.Tab("üìÅ –ò–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"):
                        doc_dir_path = gr.Textbox(
                            label="–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏",
                            placeholder="C:/Books/SocialSkills –∏–ª–∏ ./data/books",
                            info="–í—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã –±—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ"
                        )
                        
                        with gr.Row():
                            dir_chunk_size = gr.Slider(200, 2000, value=1000, step=100, label="–†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞")
                            dir_data_type = gr.Dropdown(
                                choices=["knowledge", "sft", "qa"],
                                value="knowledge",
                                label="–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö"
                            )
                        
                        process_dir_btn = gr.Button("üîÑ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é", variant="primary")
                        dir_process_result = gr.Textbox(label="–†–µ–∑—É–ª—å—Ç–∞—Ç", lines=4)
                
                gr.Markdown("---")
                gr.Markdown("### üéì –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
                
                with gr.Row():
                    datasets_info = gr.Markdown(list_document_datasets())
                    refresh_datasets_btn = gr.Button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫")
                
                with gr.Row():
                    train_doc_path = gr.Textbox(
                        label="–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É",
                        value="./data/documents/",
                        placeholder="–ü—É—Ç—å –∫ JSONL —Ñ–∞–π–ª—É –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏",
                        info="–ú–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –ø–∞–ø–∫—É - –±—É–¥–µ—Ç –≤—ã–±—Ä–∞–Ω –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ñ–∞–π–ª"
                    )
                
                with gr.Row():
                    train_doc_epochs = gr.Slider(1, 10, value=3, step=1, label="–≠–ø–æ—Ö–∏")
                    train_doc_batch = gr.Slider(1, 16, value=4, step=1, label="Batch Size")
                    train_doc_lr = gr.Number(value=2e-5, label="Learning Rate")
                
                train_doc_btn = gr.Button("üöÄ –ù–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö", variant="primary")
                train_doc_result = gr.Textbox(label="–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—É—á–µ–Ω–∏—è", lines=2)
                
                gr.Markdown("---")
                gr.Markdown("### üîç –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π (RAG)")
                
                with gr.Row():
                    rag_query = gr.Textbox(
                        label="–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å",
                        placeholder="–ö–∞–∫ –≤–µ—Å—Ç–∏ –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã?",
                        lines=1
                    )
                    rag_top_k = gr.Slider(1, 10, value=3, step=1, label="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                
                search_rag_btn = gr.Button("üîé –ò—Å–∫–∞—Ç—å", variant="secondary")
                rag_results = gr.Markdown(label="–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞")
            
            # Test Tab
            with gr.Tab("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"):
                gr.Markdown("### –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏")
                
                test_prompt = gr.Textbox(
                    label="–ü—Ä–æ–º–ø—Ç",
                    value="–ö–∞–∫ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å—Å—è –∫ —Å–ª–æ–∂–Ω–æ–º—É —Ä–∞–∑–≥–æ–≤–æ—Ä—É —Å –Ω–∞—á–∞–ª—å–Ω–∏–∫–æ–º?",
                    lines=3
                )
                
                with gr.Row():
                    test_max_tokens = gr.Slider(50, 500, value=200, label="Max Tokens")
                    test_temp = gr.Slider(0.1, 1.5, value=0.7, step=0.1, label="Temperature")
                
                test_btn = gr.Button("üîÆ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å", variant="primary")
                test_output = gr.Textbox(label="–†–µ–∑—É–ª—å—Ç–∞—Ç", lines=10)
            
            # Config Tab
            with gr.Tab("‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"):
                gr.Markdown("### –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏ –∏ –æ–±—É—á–µ–Ω–∏—è")
                
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("**–ú–æ–¥–µ–ª—å**")
                        cfg_hidden = gr.Number(value=config.model.hidden_size, label="Hidden Size")
                        cfg_layers = gr.Number(value=config.model.num_layers, label="Num Layers")
                        cfg_heads = gr.Number(value=config.model.num_heads, label="Num Heads")
                        cfg_vocab = gr.Number(value=config.model.vocab_size, label="Vocab Size")
                    
                    with gr.Column():
                        gr.Markdown("**–ì–µ–Ω–µ—Ä–∞—Ü–∏—è**")
                        cfg_temp = gr.Slider(0.1, 2.0, value=config.model.temperature, label="Temperature")
                        cfg_top_p = gr.Slider(0.1, 1.0, value=config.model.top_p, label="Top P")
                        cfg_top_k = gr.Slider(1, 100, value=config.model.top_k, label="Top K")
                
                gr.Markdown("""
                **–ü—É—Ç–∏:**
                - –ú–æ–¥–µ–ª—å: `{}`
                - –ß–µ–∫–ø–æ–∏–Ω—Ç—ã: `{}`
                - –î–∞–Ω–Ω—ã–µ: `{}`
                """.format(
                    config.model.model_path,
                    config.training.checkpoint_dir,
                    config.data_dir
                ))
        
        # Event handlers
        load_btn.click(
            load_model_handler,
            outputs=[model_info, logs_display]
        )
        
        save_btn.click(
            save_model_handler,
            outputs=[sft_result, logs_display]
        )
        
        refresh_btn.click(
            refresh_status,
            outputs=[status_display, logs_display, model_info]
        )
        
        create_data_btn.click(
            create_sample_data_handler,
            outputs=[logs_display]
        )
        
        train_sft_btn.click(
            train_sft_handler,
            inputs=[sft_data_path, sft_epochs, sft_batch, sft_lr, sft_warmup],
            outputs=[sft_result, logs_display, model_info]
        )
        
        train_dpo_btn.click(
            train_dpo_handler,
            inputs=[dpo_data_path, dpo_epochs, dpo_batch, dpo_lr, dpo_beta],
            outputs=[dpo_result, logs_display, model_info]
        )
        
        # Document training handlers
        process_files_btn.click(
            process_documents_handler,
            inputs=[doc_files, doc_chunk_size, doc_data_type],
            outputs=[doc_process_result, logs_display]
        )
        
        process_dir_btn.click(
            process_directory_handler,
            inputs=[doc_dir_path, dir_chunk_size, dir_data_type],
            outputs=[dir_process_result, logs_display]
        )
        
        refresh_datasets_btn.click(
            list_document_datasets,
            outputs=[datasets_info]
        )
        
        train_doc_btn.click(
            train_on_documents_handler,
            inputs=[train_doc_path, train_doc_epochs, train_doc_batch, train_doc_lr],
            outputs=[train_doc_result, logs_display, model_info]
        )
        
        search_rag_btn.click(
            search_rag_handler,
            inputs=[rag_query, rag_top_k],
            outputs=[rag_results]
        )
        
        test_btn.click(
            test_generation_handler,
            inputs=[test_prompt, test_max_tokens, test_temp],
            outputs=[test_output]
        )
        
        # Auto-refresh logs every 2 seconds during training
        interface.load(
            refresh_logs,
            outputs=[logs_display],
            every=2
        )
    
    return interface


# Main entry point
if __name__ == "__main__":
    print("üöÄ Starting Social Skills Model Training Interface...")
    print(f"   Device: {config.training.device}")
    print(f"   GPU Available: {torch.cuda.is_available()}")
    
    interface = create_interface()
    interface.launch(
        server_name=config.gradio_host,
        server_port=config.gradio_port,
        share=False,
        inbrowser=True
    )
